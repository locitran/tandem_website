{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15928eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PurePosixPath('/home/loci/tandem_website'),\n",
       " '5.38.0',\n",
       " '/home/loci/tandem_website/gradio_app')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import ipynbname\n",
    "import gradio as gr\n",
    "import json\n",
    "import sass \n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root = ipynbname.path().parents[1]\n",
    "sys.path.insert(0, str(root))\n",
    "# from tandem.src.utils.logger import LOGGER\n",
    "\n",
    "from src.logger import LOGGER\n",
    "\n",
    "# tandem folder\n",
    "TANDEM_DIR = os.path.join(root, 'tandem')\n",
    "GRADIO_DIR = os.path.join(root, 'gradio_app')\n",
    "TMP_DIR = os.path.join(GRADIO_DIR, 'tmp')\n",
    "JOB_DIR = os.path.join(TANDEM_DIR, 'jobs')\n",
    "\n",
    "SASS_DIR = os.path.join(GRADIO_DIR, \"sass\")\n",
    "ASSETS_DIR = os.path.join(GRADIO_DIR, \"assets\")\n",
    "\n",
    "figure_1 = os.path.join(ASSETS_DIR, 'images/figure_1.jpg')\n",
    "\n",
    "sass.compile(dirname=(str(SASS_DIR), str(ASSETS_DIR)), output_style=\"expanded\")\n",
    "sys.path.append(TANDEM_DIR)\n",
    "with open(os.path.join(ASSETS_DIR, \"main.css\")) as f:\n",
    "    custom_css = f.read()\n",
    "\n",
    "\n",
    "root, gr.__version__, os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132fa9d7",
   "metadata": {},
   "source": [
    "# MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1b2742e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('695d00d48029af87bb26679d'), acknowledged=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "db = client[\"job_db\"]\n",
    "collections = db[\"input_queue\"]\n",
    "\n",
    "# Session A has 3 jobs\n",
    "job_001 = {\n",
    "    \"status\": \"finished\",\n",
    "    \"session_id\": \"loci\",\n",
    "    \"mode\": \"Inferencing\",\n",
    "    \"inf_sav_txt\": \"P29033 Y217D\",\n",
    "    \"inf_sav_btn\": \"Uploadasdasdasdasdasdasdasd\",\n",
    "    \"model_dropdown\": \"TANDEM\",\n",
    "    \"tf_sav_txt\": \"\",\n",
    "    \"tf_sav_btn\": \"Upload\",\n",
    "    \"str_txt\": \"\",\n",
    "    \"str_btn\": \"Upload\",\n",
    "    \"job_name\": \"Tue_Dec_9_13-28-04_2025\",\n",
    "    \"email\": \"\"\n",
    "}\n",
    "\n",
    "job_002 = {\n",
    "    \"status\": \"finished\",\n",
    "    \"session_id\": \"loci\",\n",
    "    \"mode\": \"Inferencing\",\n",
    "    \"inf_sav_txt\": \"P29033 Y217D\",\n",
    "    \"inf_sav_btn\": \"Upload\",\n",
    "    \"model_dropdown\": \"TANDEM\",\n",
    "    \"tf_sav_txt\": \"\",\n",
    "    \"tf_sav_btn\": \"Upload\",\n",
    "    \"str_txt\": \"\",\n",
    "    \"str_btn\": \"Upload\",\n",
    "    \"job_name\": \"Tue_Dec_9_13-55-29_2025\",\n",
    "    \"email\": \"\"\n",
    "}\n",
    "\n",
    "job_003 = {\n",
    "    \"status\": \"finished\",\n",
    "    \"session_id\": \"abc\",\n",
    "    \"mode\": \"Inferencing\",\n",
    "    \"inf_sav_txt\": \"P29033 Y217D\",\n",
    "    \"inf_sav_btn\": \"Upload\",\n",
    "    \"model_dropdown\": \"TANDEM\",\n",
    "    \"tf_sav_txt\": \"\",\n",
    "    \"tf_sav_btn\": \"Upload\",\n",
    "    \"str_txt\": \"\",\n",
    "    \"str_btn\": \"Upload\",\n",
    "    \"job_name\": \"2025-12-15_14-58-07\",\n",
    "    \"email\": \"\"\n",
    "}\n",
    "\n",
    "job_004 = {\n",
    "    \"status\": \"finished\",\n",
    "    \"session_id\": \"abc\",\n",
    "    \"mode\": \"Transfer learning\",\n",
    "    \"inf_sav_txt\": \"P29033 Y217D\",\n",
    "    \"inf_sav_btn\": \"Upload\",\n",
    "    \"model_dropdown\": \"TANDEM\",\n",
    "    \"tf_sav_txt\": \"\",\n",
    "    \"tf_sav_btn\": \"Upload\",\n",
    "    \"str_txt\": \"\",\n",
    "    \"str_btn\": \"Upload\",\n",
    "    \"job_name\": \"2025-12-15_14-58-07\",\n",
    "    \"email\": \"\"\n",
    "}\n",
    "\n",
    "# Session B has 2 jobs\n",
    "job_101 = {\n",
    "    \"status\": \"finished\",\n",
    "    \"session_id\": \"abc\",\n",
    "    \"mode\": \"Inferencing\",\n",
    "    \"inf_sav_txt\": \"P29033 Y217D\",\n",
    "    \"inf_sav_btn\": \"Upload\",\n",
    "    \"model_dropdown\": \"TANDEM\",\n",
    "    \"tf_sav_txt\": \"\",\n",
    "    \"tf_sav_btn\": \"Upload\",\n",
    "    \"str_txt\": \"\",\n",
    "    \"str_btn\": \"Upload\",\n",
    "    \"job_name\": \"2025-12-15_15-03-01\",\n",
    "    \"email\": \"\"\n",
    "}\n",
    "\n",
    "job_102 = {\n",
    "  \"status\": \"finished\",\n",
    "  \"session_id\": \"QVP4GRh26k\",\n",
    "  \"mode\": \"Inferencing\",\n",
    "  \"SAV\": [\"O00189 R271H\",\"O00194 P138L\"],\n",
    "  \"label\": None,\n",
    "  \"model\": \"TANDEM\",\n",
    "  \"job_name\": \"2026-01-05_19-05-14\",\n",
    "  \"STR\": None,\n",
    "  \"job_start\": 1767685028.046281,\n",
    "  \"job_start_str\": \"2026-01-06_15-37-08\",\n",
    "  \"job_end\": 1767685105.7838254,\n",
    "  \"job_end_str\": \"2026-01-06_15-38-25\"\n",
    "}\n",
    "\n",
    "job_103 = {\n",
    "    \"session_id\": \"uXXF0nC3qJ\",\n",
    "    \"job_name\": \"2026-01-05_16-52-24\",\n",
    "    \"status\": \"finished\",\n",
    "    \"mode\": \"Inferencing\",\n",
    "    \"SAV\": [\n",
    "        \"O00189 R271H\",\n",
    "        \"O00194 P138L\",\n",
    "        \"O00194 A92T\",\n",
    "        \"O00204 V240I\",\n",
    "        \"O00204 L51S\",\n",
    "        \"O00206 T175A\",\n",
    "        \"O00206 Q188R\",\n",
    "        \"O00206 C246S\",\n",
    "        \"O00206 E287D\",\n",
    "        \"O00206 E287G\",\n",
    "        \"O00206 C306W\"\n",
    "    ],\n",
    "    \"label\": None,\n",
    "    \"model\": \"TANDEM\",\n",
    "    \"STR\": None,\n",
    "    \"job_start\": 1767603149.5878928,\n",
    "    \"job_start_str\": \"2026-01-05_16-52-29\",\n",
    "    \"job_end\": 1767603357.7976372,\n",
    "    \"job_end_str\": \"2026-01-05_16-55-57\"\n",
    "}\n",
    "\n",
    "job_104 = {\n",
    "    \"session_id\": \"QVP4GRh26k\",\n",
    "    \"job_name\": \"2026-01-06_20-30-19\",\n",
    "    \"status\": \"finished\",\n",
    "    \"mode\": \"Inferencing\",\n",
    "    \"SAV\": [\n",
    "        \"O00189 R271H\",\n",
    "        \"O00194 P138L\"\n",
    "    ],\n",
    "    \"label\": None,\n",
    "    \"model\": \"TANDEM\",\n",
    "    \"STR\": None,\n",
    "    \"job_start\": 1767702626.1742072,\n",
    "    \"job_start_str\": \"2026-01-06_20-30-26\",\n",
    "    \"job_end\": 1767702723.3143313,\n",
    "    \"job_end_str\": \"2026-01-06_20-32-03\"\n",
    "}\n",
    "\n",
    "collections.insert_one(job_001)\n",
    "collections.insert_one(job_002)\n",
    "collections.insert_one(job_003)\n",
    "collections.insert_one(job_004)\n",
    "collections.insert_one(job_101)\n",
    "collections.insert_one(job_102)\n",
    "collections.insert_one(job_103)\n",
    "collections.insert_one(job_104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "140ff78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tandem.src.main import run "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ca607",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c2cab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%blocks --share\n",
    "# import gradio as gr\n",
    "\n",
    "# def on_submit(text):\n",
    "#     return (\n",
    "#         f\"You typed: {text}\",\n",
    "#         gr.update(visible=False)\n",
    "#     )\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\"## Textbox Test\")\n",
    "\n",
    "#     with gr.Group() as g:\n",
    "#         textbox = gr.Textbox(\n",
    "#             label=\"Enter something\",\n",
    "#             placeholder=\"Type here and press Enter\",\n",
    "#         )\n",
    "#         output = gr.Markdown()\n",
    "\n",
    "#     textbox.submit(\n",
    "#         fn=on_submit,\n",
    "#         inputs=textbox,\n",
    "#         outputs=[output, g],\n",
    "#     )\n",
    "\n",
    "# demo.launch(server_port=7890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4304d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0053581",
   "metadata": {},
   "source": [
    "# Run the gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "404c2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pprint import pformat\n",
    "from datetime import datetime\n",
    "import json \n",
    "import pandas as pd \n",
    "import gradio as gr\n",
    "import os \n",
    "import secrets\n",
    "import gradio as gr\n",
    "import string\n",
    "from tandem.src.main import run \n",
    "import time\n",
    "from src.update_session import generate_token\n",
    "from src.update_input import upload_file, on_delete_file\n",
    "from src.web_interface import on_mode, time_zone\n",
    "from src.update_input import handle_SAV,handle_STR\n",
    "from src.update_output import multindex_DataFrame, zip_folder, on_fold, render_test_evaluation\n",
    "import shutil\n",
    "from yattag import Doc\n",
    "\n",
    "def on_session(_session_id, _param_state):\n",
    "    \"\"\"\n",
    "    1. Start session by generating new id or providing old id\n",
    "    2. Update parameter state (status and session_id)\n",
    "    3. Update dropdown of pre-trained models to include trained models from user\n",
    "        Look up all jobs of session_id to find inference jobs which were finished.\n",
    "\n",
    "    If id is not valid, no session id is recorded\n",
    "    \"\"\"\n",
    "    old_session_ids = collections.distinct(\"session_id\")\n",
    "    _session_id = _session_id.strip()\n",
    "    param_udt = _param_state.copy()\n",
    "    param_udt['status'] = None\n",
    "    param_udt[\"session_id\"] = None\n",
    "    job_dropdown_upt = gr.update(visible=False)\n",
    "    model_dropdown_udt = gr.update()\n",
    "    model_choices = [\"TANDEM\", \"TANDEM-DIMPLE for GJB2\", \"TANDEM-DIMPLE for RYR1\"]\n",
    "    session_btn_udt = gr.update(interactive=False)\n",
    "\n",
    "    # Case 1: Empty input ‚Üí Generate a new unique session ID\n",
    "    if not _session_id:\n",
    "        # loop until finding an unused ID (guaranteed uniqueness)\n",
    "        while True:\n",
    "            new_id = generate_token(length=10) \n",
    "            # if new_id overlap with old one --> redo\n",
    "            if new_id not in old_session_ids:\n",
    "                session_id_udt = gr.update(value=new_id, interactive=False)\n",
    "                session_status_udt = f\"üîÑ New session ID has been generated. <br>‚ÑπÔ∏è Please save the session ID for future reference.\"\n",
    "                param_udt[\"session_id\"] = new_id\n",
    "                break\n",
    "    # Case 2: User-provided input, check validity\n",
    "    elif _session_id not in old_session_ids:\n",
    "        session_id_udt = gr.update(value=\"\", interactive=True)\n",
    "        session_status_udt = f\"Please generate or paste a valid one.\"\n",
    "    # Case 3: Valid existing session\n",
    "    else:\n",
    "        session_id_udt = gr.update(value=_session_id, interactive=False)\n",
    "        session_status_udt = f\"‚úÖ Session resumed.\"\n",
    "\n",
    "        # List out existing jobs of this _session_id and status not None\n",
    "        existing_jobs = collections.distinct(\n",
    "            \"job_name\",\n",
    "            {\n",
    "                \"session_id\": _session_id, \n",
    "                \"status\": {\"$in\": [\"pending\", \"processing\", \"finished\"]}\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if len(existing_jobs) == 0:\n",
    "            job_dropdown_upt = gr.update(visible=False, value=None, choices=[], \n",
    "                interactive=False,label=\"No jobs in this session yet\",)\n",
    "        else:\n",
    "            first_job = existing_jobs[0]\n",
    "            param_udt = collections.find_one(\n",
    "                {'session_id': _session_id,'job_name'  : first_job,}, {\"_id\": 0}\n",
    "            )\n",
    "            job_dropdown_upt = gr.update(\n",
    "                visible=True, value=first_job, choices=existing_jobs, interactive=True, label='Old jobs',)\n",
    "\n",
    "            # List out pretrained model saved from job_name, status, and mode \n",
    "            pre_trained_models = collections.distinct(\n",
    "                \"job_name\",\n",
    "                {\n",
    "                    \"session_id\": _session_id, \n",
    "                    \"status\": \"finished\",\n",
    "                    \"mode\": \"Transfer learning\"\n",
    "                }\n",
    "            )\n",
    "            model_choices += pre_trained_models\n",
    "            model_dropdown_udt = gr.update(choices=model_choices)\n",
    "\n",
    "    return session_id_udt, session_btn_udt, session_status_udt, job_dropdown_upt, param_udt, model_dropdown_udt\n",
    "\n",
    "\n",
    "def toSAV_coords(SAVs):\n",
    "    \"\"\"\n",
    "    >>> a = ['P29033 Y217E', 'P29033 Y217F', 'P29033 Y217T']\n",
    "    >>> toSAV_coords(a)\n",
    "    ['P29033 217 Y E', 'P29033 217 Y F', 'P29033 217 Y T']\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for s in SAVs:\n",
    "        acc, wt_resid_mt = s.split()\n",
    "        wt = wt_resid_mt[0]\n",
    "        mt = wt_resid_mt[-1]\n",
    "        resid = wt_resid_mt[0+1:-1]\n",
    "        out.append(f\"{acc} {resid} {wt} {mt}\")\n",
    "    return out\n",
    "    \n",
    "def send_job(_param_state, jobs_folder):\n",
    "\n",
    "    _job_status = _param_state.get('status', None)\n",
    "\n",
    "    if _job_status != 'pending':\n",
    "        return _param_state\n",
    "\n",
    "    params_udt = _param_state.copy()\n",
    "\n",
    "    session_id = params_udt[\"session_id\"]\n",
    "    job_name   = params_udt[\"job_name\"]\n",
    "\n",
    "    params_udt[\"status\"] = \"processing\"\n",
    "    params_udt[\"job_start\"] = time.time()\n",
    "\n",
    "    # Insert initial record\n",
    "    collections.insert_one(params_udt)\n",
    "\n",
    "    # ---- Run the job (blocking) ----\n",
    "    run(\n",
    "        query=toSAV_coords(params_udt[\"SAV\"]),\n",
    "        labels=params_udt[\"label\"],\n",
    "        custom_PDB=params_udt[\"label\"],\n",
    "        job_name=f\"{session_id}/{job_name}\",\n",
    "    )\n",
    "\n",
    "    # ---- Job finished ----\n",
    "    params_udt[\"status\"] = \"finished\"\n",
    "    params_udt[\"job_end\"] = time.time()\n",
    "\n",
    "    collections.update_one(\n",
    "        {\"session_id\": session_id, \"job_name\": job_name},\n",
    "        {\"$set\": {\n",
    "            \"status\": \"finished\",\n",
    "            \"job_end\": params_udt[\"job_end\"]\n",
    "        }}\n",
    "    )\n",
    "    params_to_dump = dict(params_udt)   # copy\n",
    "    params_to_dump.pop(\"_id\", None)     # üîë remove ObjectId\n",
    "\n",
    "    with open(f\"{jobs_folder}/{session_id}/{job_name}/params.json\", \"w\") as f:\n",
    "        json.dump(params_to_dump, f, indent=4)\n",
    "\n",
    "    return params_udt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4f0b0",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d31fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, time\n",
    "import gradio as gr\n",
    "from src.web_interface import session, tandem_output, tandem_input\n",
    "from src.update_session import then_session\n",
    "from src.update_input import update_input_param\n",
    "from src.job import on_submit, on_reset, check_status\n",
    "from src.update_output import on_select_image, render_finished_job\n",
    "\n",
    "def session():\n",
    "\n",
    "    with gr.Group():\n",
    "        gr.Markdown(\"### User session\", elem_classes=\"h3\")\n",
    "        placeholder = \"Start a new session or paste an existing session ID\"\n",
    "        session_id = gr.Textbox(label=\" \", show_label=True, placeholder=placeholder,  interactive=True, show_copy_button=True, elem_classes=\"textbox\")\n",
    "        session_btn = gr.Button(\"‚ñ∂Ô∏è Start / Resume Session\", elem_classes=\"button\")\n",
    "        session_status = gr.Markdown(\"\")\n",
    "        job_dropdown = gr.Dropdown(label=\"Old jobs\", visible=False, filterable=False, allow_custom_value=False, preserved_by_key=None)\n",
    "    \n",
    "    return session_id, session_btn, session_status, job_dropdown\n",
    "\n",
    "def update_output_sections(param_state, folder):\n",
    "    \"\"\"\n",
    "    Handle output-related UI updates:\n",
    "    - output section visibility\n",
    "    - prediction table\n",
    "    - images\n",
    "    - training / evaluation artifacts\n",
    "    \"\"\"\n",
    "    _session_id = param_state.get(\"session_id\")\n",
    "    _job_status = param_state.get(\"status\")\n",
    "    _job_name   = param_state.get(\"job_name\")\n",
    "    _mode       = param_state.get(\"mode\")\n",
    "\n",
    "    # Defaults: hide everything\n",
    "    def hide_all(n):\n",
    "        return [gr.update(visible=False) for _ in range(n)]\n",
    "\n",
    "    if _job_status == \"finished\":\n",
    "        job_folder = os.path.join(folder, _session_id, _job_name)\n",
    "        return render_finished_job(_mode, job_folder)\n",
    "    else:\n",
    "        return hide_all(14)\n",
    "\n",
    "def update_status_sections(param_state):\n",
    "    \"\"\"\n",
    "    Handle submit / processing UI:\n",
    "    - input section\n",
    "    - submit section\n",
    "    - status text\n",
    "    - buttons\n",
    "    \"\"\"\n",
    "    _session_id = param_state.get(\"session_id\")\n",
    "    _job_status = param_state.get(\"status\")\n",
    "    _job_name   = param_state.get(\"job_name\")\n",
    "\n",
    "    job_start = param_state.get(\"job_start\")\n",
    "    job_end   = param_state.get(\"job_end\")\n",
    "\n",
    "    input_section_udt  = gr.update(visible=False)\n",
    "    submit_section_udt = gr.update(visible=False)\n",
    "\n",
    "    submit_status_udt  = gr.update(visible=False)\n",
    "    submit_btn_udt     = gr.update(visible=False)\n",
    "    reset_btn_udt      = gr.update(visible=False)\n",
    "\n",
    "    process_status_udt = gr.update(visible=False)\n",
    "    timer_udt          = gr.update(active=True)\n",
    "\n",
    "    if _job_status == \"finished\":\n",
    "        runtime = int(job_end - job_start)\n",
    "        msg = f\"üì¶ Payload collected for job: {_job_name}\"\n",
    "        msg += f\"\\n{json.dumps(param_state, indent=2, sort_keys=True)}\"\n",
    "        msg += f\"\\n‚úÖ Finished in {runtime}s\"\n",
    "\n",
    "        submit_status_udt  = gr.update(value=msg, visible=True)\n",
    "        process_status_udt = gr.update(visible=False)\n",
    "        timer_udt          = gr.update(active=False)\n",
    "\n",
    "    elif _job_status == \"pending\":\n",
    "        msg = f\"üì¶ Payload collected for job: {_job_name}\"\n",
    "        msg += f\"\\n{json.dumps(param_state, indent=2, sort_keys=True)}\"\n",
    "\n",
    "        submit_status_udt  = gr.update(value=msg, visible=True)\n",
    "        process_status_udt = gr.update(value=\"‚è≥ Waiting in queue...\", visible=True)\n",
    "\n",
    "    elif _job_status == \"processing\" and job_start:\n",
    "        elapsed = int(time.time() - job_start)\n",
    "        emoji = [\"‚è≥\", \"üîÑ\", \"üîÅ\", \"üîÉ\"][elapsed % 4]\n",
    "\n",
    "        msg = f\"üì¶ Payload collected for job: {_job_name}\"\n",
    "        msg += f\"\\n{json.dumps(param_state, indent=2, sort_keys=True)}\"\n",
    "\n",
    "        submit_status_udt  = gr.update(value=msg, visible=True)\n",
    "        process_status_udt = gr.update(\n",
    "            value=f\"{emoji} Model is running... {elapsed}s elapsed.\",\n",
    "            visible=True,\n",
    "        )\n",
    "\n",
    "    elif _session_id is None:\n",
    "        pass\n",
    "\n",
    "    elif _job_status is None:\n",
    "        input_section_udt  = gr.update(visible=True)\n",
    "        submit_section_udt = gr.update(visible=True)\n",
    "        submit_btn_udt     = gr.update(visible=True)\n",
    "\n",
    "    return (\n",
    "        input_section_udt,\n",
    "        submit_section_udt,\n",
    "        submit_status_udt,\n",
    "        submit_btn_udt,\n",
    "        reset_btn_udt,\n",
    "        process_status_udt,\n",
    "        timer_udt,\n",
    "    )\n",
    "\n",
    "def on_job(job_dropdown, param_state):\n",
    "    _session_id = param_state[\"session_id\"]\n",
    "    _job_name   = job_dropdown\n",
    "\n",
    "    param_udt = collections.find_one(\n",
    "        {\"session_id\": _session_id, \"job_name\": _job_name},\n",
    "        {\"_id\": 0},\n",
    "    )\n",
    "    if not param_udt:\n",
    "        raise LookupError(\"Cannot find job from on_job function\")\n",
    "\n",
    "    return param_udt\n",
    "\n",
    "def right_column(folder):\n",
    "    \n",
    "    param_state = gr.State({})\n",
    "    jobs_folder_state = gr.State(folder)\n",
    "\n",
    "    # Session UI\n",
    "    (\n",
    "        session_id, \n",
    "        session_btn, \n",
    "        session_status, \n",
    "        job_dropdown\n",
    "\n",
    "    ) = session()\n",
    "    \n",
    "    # Input UI\n",
    "    (\n",
    "        input_section,\n",
    "        mode,\n",
    "        inf_section,\n",
    "        inf_sav_txt,\n",
    "        inf_sav_file,\n",
    "        model_dropdown,\n",
    "        \n",
    "        tf_section,\n",
    "        tf_sav_txt,\n",
    "        tf_sav_file,\n",
    "\n",
    "        str_txt,\n",
    "        str_file,\n",
    "\n",
    "        job_name_txt,\n",
    "        email_txt,\n",
    "\n",
    "        submit_section,\n",
    "        submit_status,\n",
    "        process_status,\n",
    "        submit_btn,\n",
    "        reset_btn,\n",
    "\n",
    "        timer,\n",
    "\n",
    "    ) = tandem_input()\n",
    "\n",
    "    # Result UI\n",
    "    (\n",
    "        output_section,\n",
    "        inf_output_secion,\n",
    "        tf_output_secion,\n",
    "\n",
    "        pred_table,\n",
    "        image_selector,\n",
    "        image_viewer,\n",
    "\n",
    "        folds_state,\n",
    "        fold_dropdown,\n",
    "        train_box,\n",
    "        val_box,\n",
    "        test_box,\n",
    "        loss_image,\n",
    "        test_evaluation,\n",
    "\n",
    "        result_zip,\n",
    "\n",
    "    ) = tandem_output()\n",
    "\n",
    "    ################-------------Simulate session event----------------################\n",
    "    # Generate/resume session\n",
    "    session_click_event = session_btn.click(\n",
    "        fn=on_session,\n",
    "        inputs=[session_id, param_state],\n",
    "        outputs=[session_id, session_btn, session_status, job_dropdown, param_state, model_dropdown]\n",
    "    )\n",
    "    session_submit_event = session_id.submit(\n",
    "        fn=on_session,\n",
    "        inputs=[session_id, param_state],\n",
    "        outputs=[session_id, session_btn, session_status, job_dropdown, param_state, model_dropdown]\n",
    "    )\n",
    "\n",
    "    # Visualize input section\n",
    "    session_event = [session_click_event, session_submit_event]\n",
    "    for i, event in enumerate(session_event):\n",
    "        session_event[i] = event.then(\n",
    "            fn=update_status_sections,\n",
    "            inputs=[param_state],\n",
    "            outputs=[\n",
    "                input_section,\n",
    "                submit_section,\n",
    "                submit_status,\n",
    "                submit_btn,\n",
    "                reset_btn,\n",
    "                process_status,\n",
    "                timer,\n",
    "            ]).then(\n",
    "            fn=update_output_sections,\n",
    "            inputs=[param_state, jobs_folder_state],\n",
    "            outputs=[\n",
    "                output_section,\n",
    "                result_zip,\n",
    "                inf_output_secion,\n",
    "                pred_table,\n",
    "                image_selector,\n",
    "                image_viewer,\n",
    "                tf_output_secion,\n",
    "                folds_state,\n",
    "                fold_dropdown,\n",
    "                train_box,\n",
    "                val_box,\n",
    "                test_box,\n",
    "                loss_image,\n",
    "                test_evaluation,\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    ###############---input_section following job selection--------################\n",
    "    job_dropdown.select(\n",
    "        fn=on_job, inputs=[job_dropdown, param_state], outputs=[param_state]).then(\n",
    "        fn=update_status_sections, inputs=[param_state], \n",
    "            outputs=[\n",
    "                input_section,\n",
    "                submit_section,\n",
    "                submit_status,\n",
    "                submit_btn,\n",
    "                reset_btn,\n",
    "                process_status,\n",
    "                timer,\n",
    "            ]).then(\n",
    "        fn=update_output_sections, inputs=[param_state, jobs_folder_state],\n",
    "            outputs=[\n",
    "                output_section,\n",
    "                result_zip,\n",
    "                inf_output_secion,\n",
    "                pred_table,\n",
    "                image_selector,\n",
    "                image_viewer,\n",
    "                tf_output_secion,\n",
    "                folds_state,\n",
    "                fold_dropdown,\n",
    "                train_box,\n",
    "                val_box,\n",
    "                test_box,\n",
    "                loss_image,\n",
    "                test_evaluation,\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    ###############---input_section following job selection--------################\n",
    "    submit_event = submit_btn.click(\n",
    "        fn=on_submit, \n",
    "        inputs=[], \n",
    "        outputs=[submit_status, submit_btn]\n",
    "    ).then( \n",
    "        # Start timer after submission\n",
    "        fn=lambda: gr.update(active=True),\n",
    "        inputs=[],\n",
    "        outputs=timer\n",
    "    )\n",
    "\n",
    "    reset_event = reset_btn.click(\n",
    "        # Stop timer after reset\n",
    "        fn=lambda: gr.update(active=False),\n",
    "        inputs=[],\n",
    "        outputs=timer\n",
    "    ).then(\n",
    "        fn=on_reset, inputs=[param_state], \n",
    "        outputs=[\n",
    "            param_state,\n",
    "            job_dropdown,\n",
    "            input_section,\n",
    "            output_section,\n",
    "            inf_sav_txt,\n",
    "            inf_sav_file,\n",
    "            tf_sav_txt,\n",
    "            tf_sav_file,\n",
    "            str_txt,\n",
    "            str_file,\n",
    "            job_name_txt,\n",
    "            email_txt,\n",
    "            submit_status,\n",
    "            submit_btn,\n",
    "            reset_btn,\n",
    "            process_status,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    submit_event = submit_event.then(\n",
    "        fn=update_input_param,\n",
    "        inputs=[\n",
    "            mode,\n",
    "            inf_sav_txt,\n",
    "            inf_sav_file,\n",
    "            model_dropdown,\n",
    "            tf_sav_txt,\n",
    "            tf_sav_file,\n",
    "            str_txt,\n",
    "            str_file,\n",
    "            job_name_txt,\n",
    "            email_txt,\n",
    "            param_state,\n",
    "            submit_status,\n",
    "        ],\n",
    "        outputs=[param_state, input_section, submit_status, submit_btn, reset_btn, timer],\n",
    "    )\n",
    "\n",
    "    ###############--------Submission event, send job---------################\n",
    "    submit_event = submit_event.then(\n",
    "            fn=send_job,\n",
    "            inputs=[param_state, jobs_folder_state],\n",
    "            outputs=param_state,\n",
    "        )\n",
    "\n",
    "    # ###############--------Timer, report job status---------################\n",
    "    # Check job status and update submit_status\n",
    "    # timer = gr.Timer(value=1, active=False) # Timer to check result\n",
    "    timer.tick(\n",
    "        fn=check_status, inputs=[param_state, submit_status],\n",
    "        outputs=[process_status, timer, submit_status, param_state]\n",
    "    ).then( \n",
    "        # Visualize results based on param_state['status']\n",
    "        fn=update_output_sections, inputs=[param_state, jobs_folder_state],\n",
    "        outputs=[\n",
    "            output_section,\n",
    "            result_zip,\n",
    "            inf_output_secion,\n",
    "            pred_table,\n",
    "            image_selector,\n",
    "            image_viewer,\n",
    "            tf_output_secion,\n",
    "            folds_state,\n",
    "            fold_dropdown,\n",
    "            train_box,\n",
    "            val_box,\n",
    "            test_box,\n",
    "            loss_image,\n",
    "            test_evaluation,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    image_selector.change(\n",
    "        fn=on_select_image,\n",
    "        inputs=[image_selector, jobs_folder_state, param_state],\n",
    "        outputs=image_viewer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c56b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7890\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7890/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Logging into file: /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/log.txt\n",
      "@> Logging started at 2026-01-06 20:37:27.914954\n",
      "@> Job name: ImQmIA7T9l/2026-01-06_20-37-13 started at 2026-01-06 20:37:27.915265\n",
      "@> Job directory: /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13\n",
      "@> Found 5 models in /home/loci/tandem_website/tandem/models/TANDEM.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767703048.045469 2924149 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-01-06 20:37:28.093329: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "@> SAVs saved to SAVs.txt\n",
      "@> Selected feature set: ('GNM_co_rank_full', 'ANM_stiffness_chain', 'GNM_V2_full', 'GNM_V1_full', 'GNM_Eigval1_full', 'GNM_rankV2_full', 'GNM_Eigval2_full', 'GNM_rankV1_full', 'ANM_effectiveness_chain', 'SASA', 'loop_percent', 'AG1', 'Dcom', 'AG5', 'AG3', 'SSbond', 'Hbond', 'DELTA_Hbond', 'sheet_percent', 'helix_percent', 'Rg', 'IDRs', 'Lside', 'deltaLside', 'entropy', 'wtPSIC', 'deltaPSIC', 'consurf', 'ACNR', 'BLOSUM', 'ranked_MI', 'deltaPolarity', 'deltaCharge')\n",
      "@> Mapping SAVs to PDB structures...\n",
      "@> Pickle 'data/pickles/uniprot/UniprotMap-O00189.pkl' recovered.\n",
      "@> YEAH\n",
      "@> Find 3L81 OPM and Assembly...\n",
      "@> WARNING Trying to parse as mmCIF file instead\n",
      "@> WARNING Could not find _atom_site_anisotrop in lines.\n",
      "@> WARNING No anisotropic B factors found\n",
      "@> 2209 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> Pickle 'data/pickles/uniprot/UniprotMap-O00189.pkl' saved.\n",
      "@> Pickle 'data/pickles/uniprot/UniprotMap-O00194.pkl' recovered.\n",
      "@> YEAH\n",
      "@> YEAH\n",
      "@> Find 2F7S OPM and Assembly...\n",
      "@> WARNING Trying to parse as mmCIF file instead\n",
      "@> WARNING Could not find _atom_site_anisotrop in lines.\n",
      "@> WARNING No anisotropic B factors found\n",
      "@> 2958 atoms and 1 coordinate set(s) were parsed in 0.02s.\n",
      "@> Pickle 'data/pickles/uniprot/UniprotMap-O00194.pkl' saved.\n",
      "@> 3 out of 3 SAVs have been mapped to PDB in 1.0s.\n",
      "@> Uniprot2PDB map saved to /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/Uniprot2PDB.txt\n",
      "@> Computing strutural and dynamics features from PDB structures...\n",
      "@> Processing 3l81...\n",
      "@> Fixed PDB file /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/3l81-1.pdb\n",
      "@> Loading PDB /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/3l81-1.pdb...\n",
      "@> Pickle 'PDBfeatures-3l81-bas1.pkl' recovered.\n",
      "@> ANM features computed in 0.00s.\n",
      "@> 2038 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> GNM features computed in 0.01s.\n",
      "@> WARNING Traceback (most recent call last):\n",
      "  File \"/home/loci/tandem_website/tandem/src/features/PDB.py\", line 580, in calcRONNfeature\n",
      "    IDRs = ronn.calc_ronn(seq)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/tandem_website/tandem/pyRONN/ronn/ronn.py\", line 75, in calc_ronn\n",
      "    load_libRONN()\n",
      "  File \"/home/loci/tandem_website/tandem/pyRONN/ronn/ronn.py\", line 35, in load_libRONN\n",
      "    libRONN = load_library(\"libronn\", resource_filename(__name__, '.'))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/numpy/ctypeslib.py\", line 163, in load_library\n",
      "    raise OSError(\"no file with expected extension\")\n",
      "OSError: no file with expected extension\n",
      "\n",
      "@> Failed to fetch 3l81 from RCSB PDB database [Errno 13] Permission denied: '/home/loci/tandem_website/tandem/pdbfile/raw/3l81.pdb.gz'.\n",
      "@> Fetch cif file instead.\n",
      "@> Failed to fetch 3l81 from RCSB PDB database [Errno 13] Permission denied: '/home/loci/tandem_website/tandem/pdbfile/raw/3l81.cif.gz'.\n",
      "@> WARNING Traceback (most recent call last):\n",
      "  File \"/home/loci/tandem_website/tandem/src/features/PDB.py\", line 843, in calcConSurffeatures\n",
      "    f = calcConSurf(self.pdbID, chID, folder=self.job_directory, uniref90=self.uniref90)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/tandem_website/tandem/src/features/consurf.py\", line 276, in calcConSurf\n",
      "    raise ValueError(f'Cannot download {pdbID}')\n",
      "ValueError: Cannot download 3l81\n",
      "\n",
      "@> Creating mutation file for R271H in /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/3l81_A.pdb\n",
      "@> Create Mutation file in 0.33s.\n",
      "@> Pickle 'PDBfeatures-3l81-bas1.pkl' saved.\n",
      "@> PDB features: 1/3 SAVs processed [33%]\n",
      "@> Processing 2f7s...\n",
      "@> Fixed PDB file /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/2f7s-1.pdb\n",
      "@> Loading PDB /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/2f7s-1.pdb...\n",
      "@> Pickle 'PDBfeatures-2f7s-bas1.pkl' recovered.\n",
      "@> ANM features computed in 0.00s.\n",
      "@> 2887 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> GNM features computed in 0.01s.\n",
      "@> WARNING Traceback (most recent call last):\n",
      "  File \"/home/loci/tandem_website/tandem/src/features/PDB.py\", line 580, in calcRONNfeature\n",
      "    IDRs = ronn.calc_ronn(seq)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/tandem_website/tandem/pyRONN/ronn/ronn.py\", line 75, in calc_ronn\n",
      "    load_libRONN()\n",
      "  File \"/home/loci/tandem_website/tandem/pyRONN/ronn/ronn.py\", line 35, in load_libRONN\n",
      "    libRONN = load_library(\"libronn\", resource_filename(__name__, '.'))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/numpy/ctypeslib.py\", line 163, in load_library\n",
      "    raise OSError(\"no file with expected extension\")\n",
      "OSError: no file with expected extension\n",
      "\n",
      "@> Failed to fetch 2f7s from RCSB PDB database [Errno 13] Permission denied: '/home/loci/tandem_website/tandem/pdbfile/raw/2f7s.pdb.gz'.\n",
      "@> Fetch cif file instead.\n",
      "@> Failed to fetch 2f7s from RCSB PDB database [Errno 13] Permission denied: '/home/loci/tandem_website/tandem/pdbfile/raw/2f7s.cif.gz'.\n",
      "@> WARNING Traceback (most recent call last):\n",
      "  File \"/home/loci/tandem_website/tandem/src/features/PDB.py\", line 843, in calcConSurffeatures\n",
      "    f = calcConSurf(self.pdbID, chID, folder=self.job_directory, uniref90=self.uniref90)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/tandem_website/tandem/src/features/consurf.py\", line 276, in calcConSurf\n",
      "    raise ValueError(f'Cannot download {pdbID}')\n",
      "ValueError: Cannot download 2f7s\n",
      "\n",
      "@> Creating mutation file for P138L in /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/2f7s_A.pdb\n",
      "@> Create Mutation file in 0.54s.\n",
      "@> Creating mutation file for A92T in /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/2f7s_A.pdb\n",
      "@> Create Mutation file in 0.62s.\n",
      "@> 1456 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> HBplus features calculated in 0.46s.\n",
      "@> Pickle 'PDBfeatures-2f7s-bas1.pkl' saved.\n",
      "@> PDB features: 3/3 SAVs processed [100%]\n",
      "@> PDB features computed in 3.74s.\n",
      "@> Computing sequence features ...\n",
      "@> Pickle 'data/pickles/uniprot/UniprotMap-O00189.pkl' recovered.\n",
      "@> SAVs saved to /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/_temp_PolyPhen2.txt\n",
      "@> Submitting query to PolyPhen-2...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/gradio/blocks.py\", line 2235, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/gradio/blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/anyio/to_thread.py\", line 61, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/tandem_website/gradio_app/src/job.py\", line 292, in check_status\n",
      "    param_udt = collections.find_one(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/collection.py\", line 1755, in find_one\n",
      "    for result in cursor.limit(-1):\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/cursor.py\", line 1289, in __next__\n",
      "    return self.next()\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/cursor.py\", line 1265, in next\n",
      "    if len(self._data) or self._refresh():\n",
      "                          ^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/cursor.py\", line 1213, in _refresh\n",
      "    self._send_message(q)\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/cursor.py\", line 1108, in _send_message\n",
      "    response = client._run_operation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/_csot.py\", line 125, in csot_wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py\", line 1938, in _run_operation\n",
      "    return self._retryable_read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py\", line 2047, in _retryable_read\n",
      "    return self._retry_internal(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/_csot.py\", line 125, in csot_wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py\", line 2014, in _retry_internal\n",
      "    ).run()\n",
      "      ^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py\", line 2765, in run\n",
      "    return self._read() if self._is_read else self._write()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py\", line 2910, in _read\n",
      "    self._server = self._get_server()\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py\", line 2858, in _get_server\n",
      "    return self._client._select_server(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py\", line 1833, in _select_server\n",
      "    server = topology.select_server(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/topology.py\", line 409, in select_server\n",
      "    server = self._select_server(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/topology.py\", line 387, in _select_server\n",
      "    servers = self.select_servers(\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/topology.py\", line 294, in select_servers\n",
      "    server_descriptions = self._select_servers_loop(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/loci/miniconda3/envs/tandem/lib/python3.11/site-packages/pymongo/synchronous/topology.py\", line 344, in _select_servers_loop\n",
      "    raise ServerSelectionTimeoutError(\n",
      "pymongo.errors.ServerSelectionTimeoutError: mongodb:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 695cf3868029af87bb266772, topology_type: Unknown, servers: [<ServerDescription ('mongodb', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('mongodb:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n",
      "@> PolyPhen-2 is running...\n",
      "@> PolyPhen-2's output parsed.\n",
      "@> PolyPhen-2 features have been calculated in 44.67s.\n",
      "@> Searching Pfam...\n",
      "@> Processing PF00928.26...\n",
      "@> Pfam MSA for PF00928 is written as ../tandem/src/features/tmp/PF00928_full.sth.\n",
      "@> 21091 sequence(s) with 1942 residues were parsed in 0.05s.\n",
      "@> Number of columns in MSA reduced to 281.\n",
      "@> Row occupancy refinement reduced number of rows from 21091 to 19073 in 0.01s.\n",
      "@> Sequence identity refinement reduced number of rows from 19073 to 6689 in 20.95s.\n",
      "@> Mutual information matrix was calculated in 0.25s.\n",
      "@> Pickle 'data/pickles/uniprot/UniprotMap-O00189.pkl' saved.\n",
      "@> SEQ features: 1/3 SAVs processed, O00189 [33%]\n",
      "@> Pickle 'data/pickles/uniprot/UniprotMap-O00194.pkl' recovered.\n",
      "@> SAVs saved to /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/_temp_PolyPhen2.txt\n",
      "@> Submitting query to PolyPhen-2...\n",
      "@> PolyPhen-2 is running...\n",
      "@> PolyPhen-2's output parsed.\n",
      "@> PolyPhen-2 features have been calculated in 43.62s.\n",
      "@> Searching Pfam...\n",
      "@> WARNING Residue 138 is found in multiple Pfam domains ['PF00071.28', 'PF00025.27'].\n",
      "@> Processing PF00071.28...\n",
      "@> Pfam MSA for PF00071 is written as ../tandem/src/features/tmp/PF00071_full.sth.\n",
      "@> 221821 sequence(s) with 2148 residues were parsed in 0.60s.\n",
      "@> Number of columns in MSA reduced to 173.\n",
      "@> Row occupancy refinement reduced number of rows from 221821 to 188606 in 0.07s.\n",
      "@> Row occupancy refinement reduced number of rows from 188606 to 186627 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 186627 to 184956 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 184956 to 182561 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 182561 to 180710 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 180710 to 178006 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 178006 to 176284 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 176284 to 172045 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 172045 to 168853 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 168853 to 166593 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 166593 to 164540 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 164540 to 163188 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 163188 to 160613 in 0.06s.\n",
      "@> Row occupancy refinement reduced number of rows from 160613 to 157649 in 0.05s.\n",
      "@> Row occupancy refinement reduced number of rows from 157649 to 150268 in 0.05s.\n",
      "@> Row occupancy refinement reduced number of rows from 150268 to 140673 in 0.05s.\n",
      "@> Row occupancy refinement reduced number of rows from 140673 to 99870 in 0.05s.\n",
      "@> Row occupancy refinement reduced number of rows from 99870 to 12656 in 0.04s.\n",
      "@> Sequence identity refinement reduced number of rows from 12656 to 3555 in 5.09s.\n",
      "@> Mutual information matrix was calculated in 0.07s.\n",
      "@> Processing PF00025.27...\n",
      "@> Pfam MSA for PF00025 is written as ../tandem/src/features/tmp/PF00025_full.sth.\n",
      "@> 52765 sequence(s) with 2040 residues were parsed in 0.13s.\n",
      "@> WARNING PF00025.27: No sequence found in MSA for RB27B_HUMAN\n",
      "@> WARNING Residue 92 is found in multiple Pfam domains ['PF00071.28', 'PF08477.19', 'PF00025.27'].\n",
      "@> Processing PF08477.19...\n",
      "@> Pfam MSA for PF08477 is written as ../tandem/src/features/tmp/PF08477_full.sth.\n",
      "@> 10095 sequence(s) with 1554 residues were parsed in 0.02s.\n",
      "@> WARNING PF08477.19: No sequence found in MSA for RB27B_HUMAN\n",
      "@> Pickle 'data/pickles/uniprot/UniprotMap-O00194.pkl' saved.\n",
      "@> SEQ features: 3/3 SAVs processed, O00194 [100%]\n",
      "@> SEQ features computed in 145.16s.\n",
      "@> WARNING 3 missing values for feature IDRs\n",
      "@> WARNING 3 missing values for feature consurf\n",
      "@> WARNING 3 missing values for feature ACNR\n",
      "@> SAV_coords column removed from feature matrix.\n",
      "@> Feature matrix saved to /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/features.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7201306e2520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7202717f8220> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> Predictions saved to /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/predictions.csv\n",
      "@>   _mapSAVs2PDB: 1.02s (1 time(s))\n",
      "@>   _calcANMfeatures: 0.00s (2 time(s))\n",
      "@>   _calcGNMfeatures: 0.02s (2 time(s))\n",
      "@>   _createMutationfile: 1.49s (3 time(s))\n",
      "@>   _calcHBplusfeatures: 0.46s (1 time(s))\n",
      "@>   _calcPDBfeatures: 3.74s (1 time(s))\n",
      "@>   _pph2: 88.30s (2 time(s))\n",
      "@>   _calcSEQfeatures: 145.16s (1 time(s))\n",
      "@> Run time elapsed in 156.73s.\n",
      "@> Logging stopped at 2026-01-06 20:40:04.647741\n",
      "@> Closing logfile: /home/loci/tandem_website/tandem/jobs/ImQmIA7T9l/2026-01-06_20-37-13/log.txt\n"
     ]
    }
   ],
   "source": [
    "# %%blocks --share\n",
    "import gradio as gr\n",
    "from src.web_interface import build_header, build_footer, left_column\n",
    "from src.job_manager import manager_tab\n",
    "from pathlib import Path\n",
    "\n",
    "root_path=\"/TANDEM-Tsunami\"\n",
    "\n",
    "with gr.Blocks(css=custom_css,) as demo:\n",
    "    # ---------- HEADER ---------- uXXF0nC3qJ QVP4GRh26k\n",
    "    header = build_header()\n",
    "\n",
    "    # ---------- MAIN CONTENT (with tabs) ----------\n",
    "    with gr.Column(elem_id=\"main-content\"):\n",
    "        with gr.Tab(\"Home\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    left_column()\n",
    "                with gr.Column(scale=1):\n",
    "                    right_column(JOB_DIR)\n",
    "        with gr.Tab(label=\"üóÇÔ∏è Job Manager\", id='job'):\n",
    "            manager_tab()\n",
    "\n",
    "    footer_html = build_footer(root_path)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(\n",
    "    server_port=7890, \n",
    "    allowed_paths=[\n",
    "        \"../tandem/jobs\", \n",
    "        'assets/images/nthu_logo.png'\n",
    "    ],\n",
    "    root_path=root_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fcac7f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7890\n"
     ]
    }
   ],
   "source": [
    "# LOGGER.close('website.log')\n",
    "# P29033 Y217D\n",
    "# P29033 Y217D\n",
    "# http://140.114.97.207:7863\n",
    "# lN0UGkWMRm\n",
    "# FqqMduC6Tw\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5654d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7890\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7890/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import jsonyx\n",
    "import os\n",
    "import shutil\n",
    "import gradio as gr\n",
    "\n",
    "ADMIN_PASSWORD = \"yanglab\"\n",
    "JOBS_ROOT = \"/tandem/jobs\"\n",
    "\n",
    "def on_save_job(session_id, job_name, json_text):\n",
    "    status_msg_udt = \"‚ö†Ô∏è No job selected.\"\n",
    "    if not session_id or not job_name:\n",
    "        return status_msg_udt\n",
    "\n",
    "    try:\n",
    "        \n",
    "        data = json.loads(json_text) # Parse JSON\n",
    "        collections.update_one( # Update MongoDB\n",
    "            {\"session_id\": session_id, \"job_name\": job_name},\n",
    "            {\"$set\": data}\n",
    "        )\n",
    "\n",
    "        # ---- Write params.json ----\n",
    "        path = f\"{JOBS_ROOT}/{session_id}/{job_name}/params.json\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        status_msg_udt = \"‚úÖ Job updated successfully\"\n",
    "        return status_msg_udt\n",
    "\n",
    "    except Exception as e:\n",
    "        status_msg_udt = f\"‚ùå Error: {e}\"\n",
    "        return status_msg_udt\n",
    "\n",
    "def on_delete_job(session_id, job_name, df_jobs):\n",
    "    df_jobs_udt = gr.update()\n",
    "    if not session_id or not job_name:\n",
    "        status_msg_udt = f\"üóë Deleted {session_id}/{job_name}\"\n",
    "        return status_msg_udt, df_jobs_udt\n",
    "\n",
    "    try:\n",
    "        # ---- Remove from MongoDB ----\n",
    "        collections.delete_one({\"session_id\": session_id, \"job_name\": job_name})\n",
    "        # ---- Remove job folder ----\n",
    "        job_dir = f\"{JOBS_ROOT}/{session_id}/{job_name}\"\n",
    "        if os.path.exists(job_dir):\n",
    "            shutil.rmtree(job_dir)\n",
    "\n",
    "        removed_idx = df_jobs[(df_jobs['session_id'] == session_id) & (df_jobs['job_name'] == job_name)].index\n",
    "        df_jobs_udt = df_jobs.drop(removed_idx)\n",
    "\n",
    "        status_msg_udt = f\"üóë Deleted {session_id}/{job_name}\"\n",
    "        return status_msg_udt, df_jobs_udt\n",
    "\n",
    "    except Exception as e:\n",
    "        status_msg_udt = f\"‚ùå Error deleting job: {e}\"\n",
    "        return status_msg_udt, df_jobs_udt\n",
    "\n",
    "def on_refresh(status, keyword):\n",
    "    q = {}\n",
    "    if status != \"All\":\n",
    "        q[\"status\"] = status\n",
    "\n",
    "    if keyword:\n",
    "        q[\"$or\"] = [\n",
    "            {\"session_id\": {\"$regex\": keyword, \"$options\": \"i\"}},\n",
    "            {\"job_name\": {\"$regex\": keyword, \"$options\": \"i\"}}\n",
    "        ]\n",
    "\n",
    "    jobs = list(collections.find(q, {\"_id\": 0}))\n",
    "    seen = set()\n",
    "    unique_jobs = []\n",
    "\n",
    "    for j in jobs:\n",
    "        key = (j.get(\"session_id\"), j.get(\"job_name\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_jobs.append(j)\n",
    "            \n",
    "    unique_jobs = [\n",
    "        [\n",
    "            j.get(\"session_id\", \"\"),\n",
    "            j.get(\"job_name\", \"\"),\n",
    "            j.get(\"mode\", \"\"),\n",
    "            j.get(\"status\", \"\"),\n",
    "        ]\n",
    "        for j in unique_jobs\n",
    "    ]\n",
    "    params_box_udt = gr.update(value=None, lines=1)\n",
    "    status_msg_udt = gr.update(value=None)\n",
    "    return unique_jobs, params_box_udt, status_msg_udt\n",
    "\n",
    "def on_select_job(evt: gr.SelectData, df):\n",
    "    \"\"\"This parameter evt is injected automatically by Gradio\"\"\"\n",
    "\n",
    "    # ---- Normalize row index ----\n",
    "    row_idx, col_idx = evt.index\n",
    "    session_id = df.iloc[row_idx]['session_id']\n",
    "    job_name = df.iloc[row_idx]['job_name']\n",
    "\n",
    "    job = collections.find_one(\n",
    "        {\"session_id\": session_id, \"job_name\": job_name},\n",
    "        {\"_id\": 0}\n",
    "    )\n",
    "    if job:\n",
    "        params = jsonyx.dumps(job, indent=2, indent_leaves=False, separators=(\",\", \": \"))\n",
    "    else:\n",
    "        params = {}\n",
    "    \n",
    "    params_box_udt = gr.update(value=params, lines=len(job)+3)\n",
    "    status_msg_udt = gr.update(value=None)\n",
    "    return session_id, job_name, params_box_udt, status_msg_udt\n",
    "\n",
    "def on_authentication(pw):\n",
    "    if pw == ADMIN_PASSWORD:\n",
    "        authenticated_udt = True\n",
    "        password_gate_udt = gr.update(visible=False)\n",
    "        job_manager_ui = gr.update(visible=True)\n",
    "        login_msg_udt = \"‚úÖ Access granted\"\n",
    "    else:\n",
    "        authenticated_udt = False,\n",
    "        password_gate_udt = gr.update(visible=True),\n",
    "        job_manager_ui = gr.update(visible=False),\n",
    "        login_msg_udt = \"‚ùå Incorrect password\"\n",
    "    return authenticated_udt, password_gate_udt, job_manager_ui, login_msg_udt\n",
    "\n",
    "def manager_tab():\n",
    "    authenticated = gr.State(False)\n",
    "    with gr.Tab(\"üóÇÔ∏è Job Manager\"):\n",
    "        with gr.Group(visible=True) as password_gate:\n",
    "            gr.Markdown(\"### üîí Admin Authentication\")\n",
    "            password_box = gr.Textbox(type=\"password\", label=\"Enter password\")\n",
    "            login_btn = gr.Button(\"Unlock\")\n",
    "            login_msg = gr.Markdown()\n",
    "\n",
    "        with gr.Group(visible=False) as job_manager_ui:\n",
    "            gr.Markdown(\"## üóÇÔ∏è Job Manager\")\n",
    "            # ---- Filters ----\n",
    "            with gr.Row():\n",
    "                label=\"Search (session_id or job_name)\"\n",
    "                placeholder=\"Type to filter‚Ä¶\"\n",
    "                choices = [\"All\", \"pending\", \"processing\", \"finished\"]\n",
    "                search = gr.Textbox(label=label, placeholder=placeholder, scale=2)\n",
    "                status_filter = gr.Dropdown(choices=choices, value=\"All\", label=\"Status\", scale=1)\n",
    "\n",
    "            # ---- Job Table ----\n",
    "            headers = [\"session_id\",\"job_name\",\"mode\",\"status\",]\n",
    "            df_jobs = gr.Dataframe(headers=headers, interactive=False, wrap=True)\n",
    "            # ---- State: selected job ----\n",
    "            selected_session = gr.State(None)\n",
    "            selected_job = gr.State(None)\n",
    "            params_box = gr.Code(label=\"Job Parameters (Editable JSON\")\n",
    "\n",
    "            with gr.Row():\n",
    "                save_btn = gr.Button(\"üíæ Save Changes\")\n",
    "                delete_btn = gr.Button(\"üóë Delete Job\")\n",
    "            status_msg = gr.Markdown()\n",
    "\n",
    "            # =========================================================\n",
    "            # Events\n",
    "            # =========================================================\n",
    "            search.change(on_refresh,           inputs=[status_filter, search], outputs=[df_jobs, params_box, status_msg])\n",
    "            status_filter.change(on_refresh,    inputs=[status_filter, search], outputs=[df_jobs, params_box, status_msg])\n",
    "            df_jobs.select(on_select_job,       inputs=[df_jobs], outputs=[selected_session, selected_job, params_box, status_msg])\n",
    "            save_btn.click(on_save_job,         inputs=[selected_session, selected_job, params_box], outputs=status_msg)\n",
    "            delete_btn.click(on_delete_job,     inputs=[selected_session, selected_job, df_jobs], outputs=[status_msg, df_jobs])\n",
    "        password_box.submit(on_authentication,  inputs=password_box, outputs=[authenticated, password_gate, job_manager_ui, login_msg])\n",
    "        login_btn.click(on_authentication,      inputs=password_box, outputs=[authenticated, password_gate, job_manager_ui, login_msg])\n",
    "\n",
    "# %%blocks --share\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "\n",
    "root_path=\"/TANDEM-Tsunami\"\n",
    "\n",
    "with open(main_css, \"r\") as f:\n",
    "    custom_css += f.read() + '\\n'\n",
    "\n",
    "with gr.Blocks(css=custom_css,) as demo:\n",
    "\n",
    "    # ---------- MAIN CONTENT (with tabs) ----------\n",
    "    with gr.Column(elem_id=\"main-content\"):\n",
    "        manager_tab()\n",
    "\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(\n",
    "    server_port=7890, \n",
    "    allowed_paths=[\n",
    "        \"../tandem/jobs\", \n",
    "        'assets/images/nthu_logo.png'\n",
    "    ],\n",
    "    root_path=root_path,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tandem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
